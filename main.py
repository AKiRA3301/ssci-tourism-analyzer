#!/usr/bin/env python3
"""
SSCI Tourism Academic Trend Analysis System
æ—…æ¸¸å­¦æœ¯è¶‹åŠ¿åˆ†æç³»ç»Ÿ v2.0

åŠŸèƒ½ï¼š
1. é€šè¿‡åˆæ³•APIè·å–å­¦æœ¯è®ºæ–‡æ•°æ®ï¼ˆOpenAlexã€Semantic Scholarã€Crossrefï¼‰
2. å¯¼å…¥WoS/Scopusæ‰‹åŠ¨å¯¼å‡ºçš„æ–‡ä»¶
3. æ–‡æœ¬é¢„å¤„ç†ä¸NLPåˆ†æ
4. å…³é”®è¯å…±ç°ç½‘ç»œã€LDAä¸»é¢˜å»ºæ¨¡
5. å¯è§†åŒ–åˆ†æ
6. AIè¾…åŠ©ç ”ç©¶ç¼ºå£è¯†åˆ«ä¸é€‰é¢˜å»ºè®®

ä½œè€…ï¼šClaude AI Assistant
"""

import os
import sys

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from modules.data_fetcher import DataFetcher
from modules.file_importer import FileImporter
from modules.text_processor import TextProcessor
from modules.analyzer import TrendAnalyzer
from modules.visualizer import Visualizer
from modules.ai_advisor import AIAdvisor
from modules.utils import setup_logging, print_banner

import argparse
import logging

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print_banner()
    
    parser = argparse.ArgumentParser(
        description='SSCI Tourism Academic Trend Analysis System',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä»OpenAlexè·å–æ•°æ®å¹¶åˆ†æ
  python main.py --fetch --keywords "generative AI tourism" --years 2024-2026
  
  # å¯¼å…¥WoSå¯¼å‡ºæ–‡ä»¶
  python main.py --import-wos savedrecs.txt
  
  # å¯¼å…¥Scopuså¯¼å‡ºæ–‡ä»¶  
  python main.py --import-scopus scopus.csv
  
  # å®Œæ•´åˆ†ææµç¨‹
  python main.py --analyze --output results/
  
  # AIè¾…åŠ©é€‰é¢˜å»ºè®®
  python main.py --ai-suggest --focus "virtual reality tourism"
        """
    )
    
    # æ•°æ®è·å–å‚æ•°
    fetch_group = parser.add_argument_group('æ•°æ®è·å–')
    fetch_group.add_argument('--fetch', action='store_true', help='ä»å¼€æ”¾APIè·å–æ•°æ®')
    fetch_group.add_argument('--keywords', type=str, help='æœç´¢å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰')
    fetch_group.add_argument('--years', type=str, default='2024-2026', help='å¹´ä»½èŒƒå›´ï¼ˆå¦‚2024-2026ï¼‰')
    fetch_group.add_argument('--max-results', type=int, default=500, help='æœ€å¤§è·å–æ•°é‡')
    
    # æ–‡ä»¶å¯¼å…¥å‚æ•°
    import_group = parser.add_argument_group('æ–‡ä»¶å¯¼å…¥')
    import_group.add_argument('--import-wos', type=str, help='å¯¼å…¥WoSå¯¼å‡ºæ–‡ä»¶è·¯å¾„')
    import_group.add_argument('--import-scopus', type=str, help='å¯¼å…¥Scopuså¯¼å‡ºæ–‡ä»¶è·¯å¾„')
    import_group.add_argument('--import-csv', type=str, help='å¯¼å…¥é€šç”¨CSVæ–‡ä»¶')
    
    # åˆ†æå‚æ•°
    analysis_group = parser.add_argument_group('åˆ†æé€‰é¡¹')
    analysis_group.add_argument('--analyze', action='store_true', help='æ‰§è¡Œå®Œæ•´åˆ†æ')
    analysis_group.add_argument('--lda-topics', type=int, default=8, help='LDAä¸»é¢˜æ•°é‡')
    analysis_group.add_argument('--top-keywords', type=int, default=50, help='æ˜¾ç¤ºTop Nå…³é”®è¯')
    
    # AIè¾…åŠ©
    ai_group = parser.add_argument_group('AIè¾…åŠ©')
    ai_group.add_argument('--ai-suggest', action='store_true', help='AIè¾…åŠ©é€‰é¢˜å»ºè®®')
    ai_group.add_argument('--focus', type=str, help='ç ”ç©¶èšç„¦æ–¹å‘')
    ai_group.add_argument('--api-key', type=str, help='Anthropic API Keyï¼ˆå¯é€‰ï¼‰')
    
    # è¾“å‡ºå‚æ•°
    output_group = parser.add_argument_group('è¾“å‡ºé€‰é¡¹')
    output_group.add_argument('--output', type=str, default='output/', help='è¾“å‡ºç›®å½•')
    output_group.add_argument('--format', choices=['csv', 'excel', 'json'], default='csv', help='è¾“å‡ºæ ¼å¼')
    output_group.add_argument('--no-viz', action='store_true', help='ä¸ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    
    # å…¶ä»–
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--interactive', '-i', action='store_true', help='äº¤äº’æ¨¡å¼')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.verbose else logging.INFO
    setup_logging(log_level)
    logger = logging.getLogger(__name__)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output, exist_ok=True)
    
    # åˆå§‹åŒ–ç»„ä»¶
    fetcher = DataFetcher()
    importer = FileImporter()
    processor = TextProcessor()
    analyzer = TrendAnalyzer(n_topics=args.lda_topics)
    visualizer = Visualizer(output_dir=args.output)
    advisor = AIAdvisor(api_key=args.api_key)
    
    papers = []
    
    # äº¤äº’æ¨¡å¼
    if args.interactive:
        run_interactive_mode(fetcher, importer, processor, analyzer, visualizer, advisor, args.output)
        return
    
    # æ•°æ®è·å–
    if args.fetch:
        if not args.keywords:
            logger.error("è¯·ä½¿ç”¨ --keywords æŒ‡å®šæœç´¢å…³é”®è¯")
            return
        
        keywords = [k.strip() for k in args.keywords.split(',')]
        year_start, year_end = map(int, args.years.split('-'))
        
        logger.info(f"æ­£åœ¨è·å–æ•°æ®: å…³é”®è¯={keywords}, å¹´ä»½={year_start}-{year_end}")
        papers = fetcher.fetch_papers(
            keywords=keywords,
            year_start=year_start,
            year_end=year_end,
            max_results=args.max_results
        )
        logger.info(f"è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    # æ–‡ä»¶å¯¼å…¥
    if args.import_wos:
        logger.info(f"å¯¼å…¥WoSæ–‡ä»¶: {args.import_wos}")
        imported = importer.import_wos(args.import_wos)
        papers.extend(imported)
        logger.info(f"å¯¼å…¥ {len(imported)} ç¯‡è®ºæ–‡")
    
    if args.import_scopus:
        logger.info(f"å¯¼å…¥Scopusæ–‡ä»¶: {args.import_scopus}")
        imported = importer.import_scopus(args.import_scopus)
        papers.extend(imported)
        logger.info(f"å¯¼å…¥ {len(imported)} ç¯‡è®ºæ–‡")
    
    if args.import_csv:
        logger.info(f"å¯¼å…¥CSVæ–‡ä»¶: {args.import_csv}")
        imported = importer.import_csv(args.import_csv)
        papers.extend(imported)
        logger.info(f"å¯¼å…¥ {len(imported)} ç¯‡è®ºæ–‡")
    
    if not papers:
        logger.warning("æ²¡æœ‰æ•°æ®å¯åˆ†æã€‚è¯·ä½¿ç”¨ --fetch è·å–æ•°æ®æˆ– --import-* å¯¼å…¥æ–‡ä»¶")
        if not args.interactive:
            # ç”Ÿæˆæ¼”ç¤ºæ•°æ®
            logger.info("ç”Ÿæˆæ¼”ç¤ºæ•°æ®ä»¥å±•ç¤ºç³»ç»ŸåŠŸèƒ½...")
            papers = fetcher.generate_demo_data()
    
    # æ–‡æœ¬é¢„å¤„ç†
    logger.info("æ­£åœ¨è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†...")
    processed_papers = processor.process_papers(papers)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    output_file = os.path.join(args.output, f'processed_papers.{args.format}')
    processor.save_to_file(processed_papers, output_file, format=args.format)
    logger.info(f"æ•°æ®å·²ä¿å­˜è‡³: {output_file}")
    
    # æ‰§è¡Œåˆ†æ
    if args.analyze or True:  # é»˜è®¤æ‰§è¡Œåˆ†æ
        logger.info("æ­£åœ¨æ‰§è¡Œè¶‹åŠ¿åˆ†æ...")
        
        # å…³é”®è¯åˆ†æ
        keyword_stats = analyzer.analyze_keywords(processed_papers)
        analyzer.save_keyword_stats(keyword_stats, os.path.join(args.output, 'keyword_analysis.csv'))
        
        # çªå‘è¯æ£€æµ‹
        burst_words = analyzer.detect_burst_words(processed_papers)
        analyzer.save_burst_words(burst_words, os.path.join(args.output, 'burst_words.csv'))
        
        # LDAä¸»é¢˜å»ºæ¨¡
        topics = analyzer.lda_topic_modeling(processed_papers)
        analyzer.save_topics(topics, os.path.join(args.output, 'lda_topics.txt'))
        
        # ç ”ç©¶ç¼ºå£åˆ†æ
        gaps = analyzer.identify_research_gaps(processed_papers)
        analyzer.save_gaps(gaps, os.path.join(args.output, 'research_gaps.txt'))
        
        # å¯è§†åŒ–
        if not args.no_viz:
            logger.info("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            visualizer.plot_keyword_trends(keyword_stats)
            visualizer.plot_cooccurrence_network(processed_papers)
            visualizer.plot_yearly_heatmap(processed_papers)
            visualizer.plot_topic_distribution(topics)
            visualizer.plot_citation_analysis(processed_papers)
            logger.info(f"å›¾è¡¨å·²ä¿å­˜è‡³: {args.output}")
    
    # AIè¾…åŠ©å»ºè®®
    if args.ai_suggest:
        logger.info("æ­£åœ¨ç”ŸæˆAIè¾…åŠ©é€‰é¢˜å»ºè®®...")
        suggestions = advisor.generate_suggestions(
            papers=processed_papers,
            gaps=gaps if 'gaps' in locals() else None,
            focus_area=args.focus
        )
        advisor.save_suggestions(suggestions, os.path.join(args.output, 'ai_suggestions.md'))
        logger.info(f"AIå»ºè®®å·²ä¿å­˜è‡³: {args.output}/ai_suggestions.md")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    logger.info("æ­£åœ¨ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    generate_report(
        papers=processed_papers,
        keyword_stats=keyword_stats if 'keyword_stats' in locals() else None,
        burst_words=burst_words if 'burst_words' in locals() else None,
        topics=topics if 'topics' in locals() else None,
        gaps=gaps if 'gaps' in locals() else None,
        output_dir=args.output
    )
    
    logger.info("=" * 60)
    logger.info("åˆ†æå®Œæˆï¼")
    logger.info(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(args.output)}")
    logger.info("=" * 60)


def run_interactive_mode(fetcher, importer, processor, analyzer, visualizer, advisor, output_dir):
    """äº¤äº’å¼è¿è¡Œæ¨¡å¼"""
    print("\n" + "=" * 60)
    print("ğŸ“š SSCIæ—…æ¸¸å­¦æœ¯è¶‹åŠ¿åˆ†æç³»ç»Ÿ - äº¤äº’æ¨¡å¼")
    print("=" * 60)
    
    papers = []
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œï¼š")
        print("1. ä»OpenAlexè·å–æ•°æ®")
        print("2. å¯¼å…¥æœ¬åœ°æ–‡ä»¶ï¼ˆWoS/Scopus/CSVï¼‰")
        print("3. åŠ è½½æ¼”ç¤ºæ•°æ®")
        print("4. æ‰§è¡Œå…³é”®è¯åˆ†æ")
        print("5. æ‰§è¡ŒLDAä¸»é¢˜å»ºæ¨¡")
        print("6. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        print("7. AIè¾…åŠ©é€‰é¢˜å»ºè®®")
        print("8. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š")
        print("9. æŸ¥çœ‹å½“å‰æ•°æ®ç»Ÿè®¡")
        print("0. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-9): ").strip()
        
        if choice == '0':
            print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break
        
        elif choice == '1':
            keywords = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰: ").strip()
            if keywords:
                keywords_list = [k.strip() for k in keywords.split(',')]
                years = input("è¯·è¾“å…¥å¹´ä»½èŒƒå›´ï¼ˆå¦‚2024-2026ï¼Œç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: ").strip() or "2024-2026"
                year_start, year_end = map(int, years.split('-'))
                max_results = int(input("æœ€å¤§è·å–æ•°é‡ï¼ˆç›´æ¥å›è½¦é»˜è®¤500ï¼‰: ").strip() or "500")
                
                print(f"\næ­£åœ¨è·å–æ•°æ®...")
                new_papers = fetcher.fetch_papers(keywords_list, year_start, year_end, max_results)
                papers.extend(new_papers)
                print(f"âœ“ è·å–åˆ° {len(new_papers)} ç¯‡è®ºæ–‡ï¼Œå½“å‰å…± {len(papers)} ç¯‡")
        
        elif choice == '2':
            file_type = input("æ–‡ä»¶ç±»å‹ï¼ˆwos/scopus/csvï¼‰: ").strip().lower()
            file_path = input("æ–‡ä»¶è·¯å¾„: ").strip()
            
            if os.path.exists(file_path):
                if file_type == 'wos':
                    new_papers = importer.import_wos(file_path)
                elif file_type == 'scopus':
                    new_papers = importer.import_scopus(file_path)
                else:
                    new_papers = importer.import_csv(file_path)
                papers.extend(new_papers)
                print(f"âœ“ å¯¼å…¥ {len(new_papers)} ç¯‡è®ºæ–‡ï¼Œå½“å‰å…± {len(papers)} ç¯‡")
            else:
                print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        
        elif choice == '3':
            papers = fetcher.generate_demo_data()
            print(f"âœ“ å·²åŠ è½½ {len(papers)} ç¯‡æ¼”ç¤ºæ•°æ®")
        
        elif choice == '4':
            if not papers:
                print("âŒ è¯·å…ˆè·å–æˆ–å¯¼å…¥æ•°æ®")
                continue
            processed = processor.process_papers(papers)
            stats = analyzer.analyze_keywords(processed)
            burst = analyzer.detect_burst_words(processed)
            analyzer.save_keyword_stats(stats, os.path.join(output_dir, 'keyword_analysis.csv'))
            analyzer.save_burst_words(burst, os.path.join(output_dir, 'burst_words.csv'))
            print("âœ“ å…³é”®è¯åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜")
            print("\nğŸ“Š Top 10 é«˜é¢‘å…³é”®è¯ï¼š")
            for i, (kw, freq) in enumerate(list(stats.items())[:10], 1):
                print(f"  {i}. {kw}: {freq}")
        
        elif choice == '5':
            if not papers:
                print("âŒ è¯·å…ˆè·å–æˆ–å¯¼å…¥æ•°æ®")
                continue
            n_topics = int(input("ä¸»é¢˜æ•°é‡ï¼ˆé»˜è®¤8ï¼‰: ").strip() or "8")
            analyzer.n_topics = n_topics
            processed = processor.process_papers(papers)
            topics = analyzer.lda_topic_modeling(processed)
            analyzer.save_topics(topics, os.path.join(output_dir, 'lda_topics.txt'))
            print("âœ“ LDAä¸»é¢˜å»ºæ¨¡å®Œæˆ")
            print("\nğŸ“š å‘ç°çš„ç ”ç©¶ä¸»é¢˜ï¼š")
            for i, topic in enumerate(topics, 1):
                print(f"  ä¸»é¢˜{i}: {', '.join(topic['keywords'][:5])}")
        
        elif choice == '6':
            if not papers:
                print("âŒ è¯·å…ˆè·å–æˆ–å¯¼å…¥æ•°æ®")
                continue
            processed = processor.process_papers(papers)
            stats = analyzer.analyze_keywords(processed)
            topics = analyzer.lda_topic_modeling(processed)
            
            print("æ­£åœ¨ç”Ÿæˆå›¾è¡¨...")
            visualizer.plot_keyword_trends(stats)
            visualizer.plot_cooccurrence_network(processed)
            visualizer.plot_yearly_heatmap(processed)
            visualizer.plot_topic_distribution(topics)
            visualizer.plot_citation_analysis(processed)
            print(f"âœ“ æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜è‡³: {output_dir}")
        
        elif choice == '7':
            if not papers:
                print("âŒ è¯·å…ˆè·å–æˆ–å¯¼å…¥æ•°æ®")
                continue
            focus = input("ç ”ç©¶èšç„¦æ–¹å‘ï¼ˆå¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡ï¼‰: ").strip() or None
            processed = processor.process_papers(papers)
            gaps = analyzer.identify_research_gaps(processed)
            
            print("\næ­£åœ¨ç”ŸæˆAIè¾…åŠ©å»ºè®®...")
            suggestions = advisor.generate_suggestions(processed, gaps, focus)
            advisor.save_suggestions(suggestions, os.path.join(output_dir, 'ai_suggestions.md'))
            print(f"âœ“ AIå»ºè®®å·²ä¿å­˜è‡³: {output_dir}/ai_suggestions.md")
            print("\n" + "=" * 50)
            print(suggestions[:2000] + "..." if len(suggestions) > 2000 else suggestions)
        
        elif choice == '8':
            if not papers:
                print("âŒ è¯·å…ˆè·å–æˆ–å¯¼å…¥æ•°æ®")
                continue
            processed = processor.process_papers(papers)
            stats = analyzer.analyze_keywords(processed)
            burst = analyzer.detect_burst_words(processed)
            topics = analyzer.lda_topic_modeling(processed)
            gaps = analyzer.identify_research_gaps(processed)
            
            generate_report(processed, stats, burst, topics, gaps, output_dir)
            print(f"âœ“ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_dir}/analysis_report.md")
        
        elif choice == '9':
            print(f"\nğŸ“Š å½“å‰æ•°æ®ç»Ÿè®¡ï¼š")
            print(f"  è®ºæ–‡æ€»æ•°: {len(papers)}")
            if papers:
                years = [p.get('year', 0) for p in papers if p.get('year')]
                if years:
                    print(f"  å¹´ä»½èŒƒå›´: {min(years)} - {max(years)}")
                journals = set(p.get('journal', '') for p in papers if p.get('journal'))
                print(f"  æœŸåˆŠæ•°é‡: {len(journals)}")
                with_abstract = sum(1 for p in papers if p.get('abstract'))
                print(f"  æœ‰æ‘˜è¦çš„è®ºæ–‡: {with_abstract}")


def generate_report(papers, keyword_stats, burst_words, topics, gaps, output_dir):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    report = []
    report.append("# SSCIæ—…æ¸¸å­¦æœ¯è¶‹åŠ¿åˆ†ææŠ¥å‘Š")
    report.append(f"\n**ç”Ÿæˆæ—¶é—´**: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"\n**åˆ†æè®ºæ–‡æ•°**: {len(papers)}")
    
    report.append("\n---\n")
    report.append("## 1. æ•°æ®æ¦‚è§ˆ\n")
    
    if papers:
        years = [p.get('year', 0) for p in papers if p.get('year')]
        if years:
            report.append(f"- å¹´ä»½èŒƒå›´: {min(years)} - {max(years)}")
        
        journals = {}
        for p in papers:
            j = p.get('journal', 'Unknown')
            journals[j] = journals.get(j, 0) + 1
        
        report.append(f"- æ¶‰åŠæœŸåˆŠ: {len(journals)} ç§")
        report.append("\n### ä¸»è¦æœŸåˆŠåˆ†å¸ƒ\n")
        for j, count in sorted(journals.items(), key=lambda x: -x[1])[:10]:
            report.append(f"- {j}: {count} ç¯‡")
    
    if keyword_stats:
        report.append("\n---\n")
        report.append("## 2. å…³é”®è¯åˆ†æ\n")
        report.append("### 2.1 é«˜é¢‘å…³é”®è¯ Top 20\n")
        report.append("| æ’å | å…³é”®è¯ | é¢‘æ¬¡ |")
        report.append("|------|--------|------|")
        for i, (kw, freq) in enumerate(list(keyword_stats.items())[:20], 1):
            report.append(f"| {i} | {kw} | {freq} |")
    
    if burst_words:
        report.append("\n### 2.2 çªå‘è¯ï¼ˆBurst Wordsï¼‰\n")
        report.append("*çªå‘è¯è¡¨ç¤ºè¿‘æœŸå¿«é€Ÿå¢é•¿çš„ç ”ç©¶çƒ­ç‚¹*\n")
        report.append("| å…³é”®è¯ | å¢é•¿ç‡ | è¶‹åŠ¿ |")
        report.append("|--------|--------|------|")
        for bw in burst_words[:15]:
            trend = "ğŸ“ˆ" if bw.get('growth_rate', 0) > 0 else "ğŸ“‰"
            report.append(f"| {bw['keyword']} | {bw.get('growth_rate', 0):.1%} | {trend} |")
    
    if topics:
        report.append("\n---\n")
        report.append("## 3. LDAä¸»é¢˜å»ºæ¨¡ç»“æœ\n")
        for i, topic in enumerate(topics, 1):
            report.append(f"\n### ä¸»é¢˜ {i}: {topic.get('label', 'Unknown')}")
            report.append(f"**æ ¸å¿ƒå…³é”®è¯**: {', '.join(topic['keywords'][:8])}")
            report.append(f"\n**ä¸»é¢˜æè¿°**: {topic.get('description', 'å¾…è¡¥å……')}\n")
    
    if gaps:
        report.append("\n---\n")
        report.append("## 4. ç ”ç©¶ç¼ºå£è¯†åˆ«\n")
        report.append("*åŸºäº\"Limitations\"å’Œ\"Future Research\"æ–‡æœ¬æŒ–æ˜*\n")
        for i, gap in enumerate(gaps, 1):
            report.append(f"\n### ç¼ºå£ {i}: {gap['title']}")
            report.append(f"- **è¯†åˆ«æ¥æº**: {gap.get('source_count', 'N/A')} ç¯‡è®ºæ–‡æåŠ")
            report.append(f"- **ç ”ç©¶æœºä¼š**: {gap.get('opportunity', 'å¾…åˆ†æ')}")
    
    report.append("\n---\n")
    report.append("## 5. é€‰é¢˜å»ºè®®\n")
    report.append("""
åŸºäºä»¥ä¸Šåˆ†æï¼Œå»ºè®®å…³æ³¨ä»¥ä¸‹ç ”ç©¶æ–¹å‘ï¼š

1. **æ–°å…´æŠ€æœ¯åº”ç”¨**: ç»“åˆçªå‘è¯ä¸­çš„æŠ€æœ¯å…³é”®è¯ï¼ˆå¦‚AIã€VRã€IoTï¼‰ï¼Œæ¢ç´¢å…¶åœ¨æ—…æ¸¸é¢†åŸŸçš„åˆ›æ–°åº”ç”¨
2. **äº¤å‰ç ”ç©¶**: å…³æ³¨é«˜é¢‘è¯å…±ç°ç½‘ç»œä¸­çš„è·¨å­¦ç§‘ç»„åˆï¼Œå¦‚"å¯æŒç»­å‘å±•+æ•°å­—åŒ–è½¬å‹"
3. **å¡«è¡¥ç¼ºå£**: é’ˆå¯¹ç ”ç©¶ç¼ºå£éƒ¨åˆ†çš„æ–¹å‘ï¼Œè®¾è®¡é’ˆå¯¹æ€§ç ”ç©¶
4. **æ–¹æ³•è®ºåˆ›æ–°**: è€ƒè™‘é‡‡ç”¨æ··åˆç ”ç©¶æ–¹æ³•æˆ–å¤§æ•°æ®åˆ†ææ–¹æ³•

### å†™ä½œå»ºè®®

åœ¨æ–¹æ³•è®ºéƒ¨åˆ†ï¼Œå¯ä»¥å†™ï¼š
> "æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºPythonçš„å¤šé˜¶æ®µæ–‡æœ¬æŒ–æ˜æŠ€æœ¯ï¼ˆText Miningï¼‰ï¼Œå¯¹Web of Scienceæ•°æ®åº“è¿‘ä¸‰å¹´
> çš„XXXç¯‡æ—…æ¸¸ç±»SSCIè®ºæ–‡è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ¼”åŒ–è·¯å¾„åˆ†æï¼Œè¯†åˆ«å‡ºXXä¸ªæ ¸å¿ƒç ”ç©¶ä¸»é¢˜å’ŒXXä¸ªæ½œåœ¨
> ç ”ç©¶ç¼ºå£ã€‚"
""")
    
    report.append("\n---\n")
    report.append("## é™„å½•\n")
    report.append("### æ•°æ®æ–‡ä»¶æ¸…å•\n")
    report.append("- `processed_papers.csv` - é¢„å¤„ç†åçš„è®ºæ–‡æ•°æ®")
    report.append("- `keyword_analysis.csv` - å…³é”®è¯ç»Ÿè®¡")
    report.append("- `burst_words.csv` - çªå‘è¯åˆ—è¡¨")
    report.append("- `lda_topics.txt` - LDAä¸»é¢˜è¯¦æƒ…")
    report.append("- `research_gaps.txt` - ç ”ç©¶ç¼ºå£")
    report.append("- `ai_suggestions.md` - AIè¾…åŠ©å»ºè®®")
    report.append("\n### å¯è§†åŒ–å›¾è¡¨\n")
    report.append("- `keyword_trends.png` - å…³é”®è¯è¶‹åŠ¿å›¾")
    report.append("- `cooccurrence_network.png` - å…³é”®è¯å…±ç°ç½‘ç»œ")
    report.append("- `yearly_heatmap.png` - å¹´åº¦çƒ­åŠ›å›¾")
    report.append("- `topic_distribution.png` - ä¸»é¢˜åˆ†å¸ƒå›¾")
    report.append("- `citation_analysis.png` - å¼•ç”¨åˆ†æå›¾")
    
    report_text = '\n'.join(report)
    
    with open(os.path.join(output_dir, 'analysis_report.md'), 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    return report_text


if __name__ == '__main__':
    main()
